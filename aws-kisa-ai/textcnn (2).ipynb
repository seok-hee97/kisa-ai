{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bee88d-f024-4df3-b706-ec359b4ddf78",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23fce74-cfd3-43b0-b91b-b2d98e9fd1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ac0c8f-b3ce-4335-bb2b-c37cbf1ac4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.11.0 imblearn-0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7318b97-f1da-4cf1-9bc2-c751039f668e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69385473-e5f8-4605-a753-f5f8b6f9a9ac",
   "metadata": {},
   "source": [
    "### [reference]\n",
    "\n",
    "- Getting started with Text Processing   \n",
    "https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049ed7b0-4a76-45e2-b6f1-2a154601952d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda7aaaa-ae31-45f0-b6d2-80bea79e2c24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/kisa-ai\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "print(os.getcwd())  # 현재 작업 디렉토리 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18677ad5-4925-46c1-a15c-b0cf16239a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# import spacy\n",
    "import string\n",
    "\n",
    "df= pd.read_csv('../dataset/A-Track-clean.csv')\n",
    "# df = pd.read_csv('A-dataset-clean.csv')\n",
    "# df = pd.read_csv('/root/kisa-ai/A-dataset-clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8cd2dc7-6704-442c-800c-1e84b3d1e716",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>payload</th>\n",
       "      <th>label_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>get /forum1_professionnel.asp?n=/.\\\\\\\"./.\\\\\\\"....</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>post /owa/auth/logon.aspx?replacecurrent=1\") a...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>get /goods/goods_search?display_type=list&amp;arr_...</td>\n",
       "      <td>SQL_Injection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>get / http/1.1\\r\\n\\r\\n</td>\n",
       "      <td>HOST_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>get /sub_04_1_read.php?page=1&amp;id=31);select * ...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            payload  \\\n",
       "0           0  get /forum1_professionnel.asp?n=/.\\\\\\\"./.\\\\\\\"....   \n",
       "1           1  post /owa/auth/logon.aspx?replacecurrent=1\") a...   \n",
       "2           2  get /goods/goods_search?display_type=list&arr_...   \n",
       "3           3                             get / http/1.1\\r\\n\\r\\n   \n",
       "4           4  get /sub_04_1_read.php?page=1&id=31);select * ...   \n",
       "\n",
       "           label_action  \n",
       "0  System_Cmd_Execution  \n",
       "1  System_Cmd_Execution  \n",
       "2         SQL_Injection  \n",
       "3             HOST_Scan  \n",
       "4  System_Cmd_Execution  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ed03a192-33bf-4964-a7ae-92837421cd22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'payload', 'label_action'], dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d705348-f052-4321-902e-5eb85c090400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vulnerability_Scan               16867\n",
       "System_Cmd_Execution              9807\n",
       "HOST_Scan                         6315\n",
       "Path_Disclosure                   4775\n",
       "SQL_Injection                     3395\n",
       "Cross_Site_Scripting              1348\n",
       "Automatically_Searching_Infor     1119\n",
       "Leakage_Through_NW                 956\n",
       "Directory_Indexing                 418\n",
       "Name: label_action, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13b5465b-9f56-4fab-b90c-c16a888f92f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['payload'] = df['payload'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852eed74-525f-43cd-a0d3-1286ccd12049",
   "metadata": {},
   "source": [
    "#### Removal of HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a95bdda9-5995-4fb6-9c5f-b6a3c8a6c751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47cb3706-be47-4545-9828-cbd96a380e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"payload\"] = df[\"payload\"].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0766810d-b097-43fd-8aa9-e697cd60e7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>payload</th>\n",
       "      <th>label_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5063</th>\n",
       "      <td>5063</td>\n",
       "      <td>get / http/1.0\\r\\n\\r\\n</td>\n",
       "      <td>HOST_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41742</th>\n",
       "      <td>41742</td>\n",
       "      <td>post /admin/login.php http/1.0\\r\\nhost: 10.0.1...</td>\n",
       "      <td>Vulnerability_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25731</th>\n",
       "      <td>25731</td>\n",
       "      <td>get /phpinfo.php3 http/1.1\\r\\nconnection: keep...</td>\n",
       "      <td>Path_Disclosure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28755</th>\n",
       "      <td>28755</td>\n",
       "      <td>get /goods/goods_search?display_type=list&amp;arr_...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39654</th>\n",
       "      <td>39654</td>\n",
       "      <td>post /admin/login.php http/1.0\\r\\nhost: 10.0.1...</td>\n",
       "      <td>Vulnerability_Scan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            payload  \\\n",
       "5063         5063                             get / http/1.0\\r\\n\\r\\n   \n",
       "41742       41742  post /admin/login.php http/1.0\\r\\nhost: 10.0.1...   \n",
       "25731       25731  get /phpinfo.php3 http/1.1\\r\\nconnection: keep...   \n",
       "28755       28755  get /goods/goods_search?display_type=list&arr_...   \n",
       "39654       39654  post /admin/login.php http/1.0\\r\\nhost: 10.0.1...   \n",
       "\n",
       "               label_action  \n",
       "5063              HOST_Scan  \n",
       "41742    Vulnerability_Scan  \n",
       "25731       Path_Disclosure  \n",
       "28755  System_Cmd_Execution  \n",
       "39654    Vulnerability_Scan  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc587df1-c89e-483b-9e15-e3ede7c580dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Unnamed: 0':'Log_Number'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703bbdf-e894-4c06-98a3-6809c478981f",
   "metadata": {},
   "source": [
    "#### Removal of Punctuations(구두점 제거)(특수기호 같은거)\n",
    "\n",
    "One another common text preprocessing technique is to remove the punctuations from the text data.   \n",
    "This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.   \n",
    "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string.   \n",
    "punctuation in python contains the following punctuation symbols   \n",
    "\n",
    "!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`   \n",
    "\n",
    "We can add or remove more punctuations as per our need.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eede7bac-fa68-4d09-99d6-c06f230575c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log_Number</th>\n",
       "      <th>payload</th>\n",
       "      <th>label_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>get forum1professionnelaspnbootiniampnn100ampp...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>post owaauthlogonaspxreplacecurrent1 and 9294u...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>get goodsgoodssearchdisplaytypelistarrsearchli...</td>\n",
       "      <td>SQL_Injection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>get  http11rnrn</td>\n",
       "      <td>HOST_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>get sub041readphppage1id31select  from generat...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Log_Number                                            payload  \\\n",
       "0           0  get forum1professionnelaspnbootiniampnn100ampp...   \n",
       "1           1  post owaauthlogonaspxreplacecurrent1 and 9294u...   \n",
       "2           2  get goodsgoodssearchdisplaytypelistarrsearchli...   \n",
       "3           3                                    get  http11rnrn   \n",
       "4           4  get sub041readphppage1id31select  from generat...   \n",
       "\n",
       "           label_action  \n",
       "0  System_Cmd_Execution  \n",
       "1  System_Cmd_Execution  \n",
       "2         SQL_Injection  \n",
       "3             HOST_Scan  \n",
       "4  System_Cmd_Execution  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df['payload'] = df['payload'].apply(lambda text: remove_punctuation(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5692f1be-81b7-4765-8499-b3e0f7c07a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('A-Track-cleanplus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b38c6-a946-4b70-b687-f8ae31b6c1d2",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/mlwhiz/learning-text-classification-textcnn/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30c92b-ac74-4e60-a8c7-d9ccdc76cc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7c033c5-8067-4deb-9307-3ade1701fa49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8451bf90-ee19-4ed1-bee5-644da479c7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>payload</th>\n",
       "      <th>label_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>get /forum1_professionnel.asp?n=/.\\\\\\\"./.\\\\\\\"....</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>post /owa/auth/logon.aspx?replacecurrent=1\") a...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>get /goods/goods_search?display_type=list&amp;arr_...</td>\n",
       "      <td>SQL_Injection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>get / http/1.1\\r\\n\\r\\n</td>\n",
       "      <td>HOST_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>get /sub_04_1_read.php?page=1&amp;id=31);select * ...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            payload  \\\n",
       "0           0  get /forum1_professionnel.asp?n=/.\\\\\\\"./.\\\\\\\"....   \n",
       "1           1  post /owa/auth/logon.aspx?replacecurrent=1\") a...   \n",
       "2           2  get /goods/goods_search?display_type=list&arr_...   \n",
       "3           3                             get / http/1.1\\r\\n\\r\\n   \n",
       "4           4  get /sub_04_1_read.php?page=1&id=31);select * ...   \n",
       "\n",
       "           label_action  \n",
       "0  System_Cmd_Execution  \n",
       "1  System_Cmd_Execution  \n",
       "2         SQL_Injection  \n",
       "3             HOST_Scan  \n",
       "4  System_Cmd_Execution  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8cafa1f-7049-45cf-b149-42dc017c65c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 요 방법도 있음\n",
    "\n",
    "# Some preprocesssing that will be common to all the text classification methods you will see. \n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def remove_html(x):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5eb4b90-4724-4f52-b07c-47070d9653c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['payload'] = data['payload'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3387b1f4-3465-4d3c-9193-4f3e33a3cab8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>payload</th>\n",
       "      <th>label_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12245</th>\n",
       "      <td>12245</td>\n",
       "      <td>post /controller/login.php http/1.1\\r\\ncontent...</td>\n",
       "      <td>Vulnerability_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42029</th>\n",
       "      <td>42029</td>\n",
       "      <td>get /company/about http/1.1\\r\\ncache-control: ...</td>\n",
       "      <td>Leakage_Through_NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43916</th>\n",
       "      <td>43916</td>\n",
       "      <td>get /goods/goods_search?display_type=list&amp;arr_...</td>\n",
       "      <td>Vulnerability_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>get /data/skin/responsive_ver1_default_gl/comm...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13625</th>\n",
       "      <td>13625</td>\n",
       "      <td>get /dir_thatware/config.php?root_path=http://...</td>\n",
       "      <td>Vulnerability_Scan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            payload  \\\n",
       "12245       12245  post /controller/login.php http/1.1\\r\\ncontent...   \n",
       "42029       42029  get /company/about http/1.1\\r\\ncache-control: ...   \n",
       "43916       43916  get /goods/goods_search?display_type=list&arr_...   \n",
       "71             71  get /data/skin/responsive_ver1_default_gl/comm...   \n",
       "13625       13625  get /dir_thatware/config.php?root_path=http://...   \n",
       "\n",
       "               label_action  \n",
       "12245    Vulnerability_Scan  \n",
       "42029    Leakage_Through_NW  \n",
       "43916    Vulnerability_Scan  \n",
       "71     System_Cmd_Execution  \n",
       "13625    Vulnerability_Scan  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "508ebadb-7aa6-49f7-baa1-46b4273948c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['payload'] = data['payload'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13e99267-13c1-4bec-b920-e52a8022966e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>payload</th>\n",
       "      <th>label_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>get /forum1_professionnel.asp?n=/.\\\\\\\"./.\\\\\\\"....</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>post /owa/auth/logon.aspx?replacecurrent=1\") a...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>get /goods/goods_search?display_type=list&amp;arr_...</td>\n",
       "      <td>SQL_Injection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>get / http/1.1\\r\\n\\r\\n</td>\n",
       "      <td>HOST_Scan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>get /sub_04_1_read.php?page=1&amp;id=31);select * ...</td>\n",
       "      <td>System_Cmd_Execution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            payload  \\\n",
       "0           0  get /forum1_professionnel.asp?n=/.\\\\\\\"./.\\\\\\\"....   \n",
       "1           1  post /owa/auth/logon.aspx?replacecurrent=1\") a...   \n",
       "2           2  get /goods/goods_search?display_type=list&arr_...   \n",
       "3           3                             get / http/1.1\\r\\n\\r\\n   \n",
       "4           4  get /sub_04_1_read.php?page=1&id=31);select * ...   \n",
       "\n",
       "           label_action  \n",
       "0  System_Cmd_Execution  \n",
       "1  System_Cmd_Execution  \n",
       "2         SQL_Injection  \n",
       "3             HOST_Scan  \n",
       "4  System_Cmd_Execution  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77598a4c-bf6e-4b5e-986a-f9af42cdbacd",
   "metadata": {},
   "source": [
    "[kaggle CNN model Architecture]   \n",
    "https://www.kaggle.com/code/kadhambari/multi-class-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "75c6461c-c0a8-44a8-b1a0-b4513ac5043e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.9/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 9) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 62\u001b[0m\n\u001b[1;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Use softmax once\u001b[39;00m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     60\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 62\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Model evaluation\u001b[39;00m\n\u001b[1;32m     68\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5zctu9p7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.9/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 9) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# to be imported\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# Load your data (replace with your data loading logic)\n",
    "data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "X = pad_sequences(X, maxlen=201)\n",
    "y = data['label_encoded'].values\n",
    "\n",
    "# Shuffle and split the data while maintaining class balance\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Define model parameters\n",
    "max_features = 1000\n",
    "maxlen = 201\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "# CNN with max pooling implementation\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Use softmax once\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_split=0.1)\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test, y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn-kaggle-c.h5')\n",
    "print('Model saved as textcnn-kaggle-c.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8ca65ae-fc9a-4b6f-a4c4-63aec3ec1ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Build model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.9/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 1) and (32, 9) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 85\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# model.add(Activation('softmax'))\u001b[39;00m\n\u001b[1;32m     81\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     82\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 85\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Model evaluation\u001b[39;00m\n\u001b[1;32m     92\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5zctu9p7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.9/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 1) and (32, 9) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# to be imported\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# Load your data (replace with your data loading logic)\n",
    "data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "# max_sequence_length = max(len(x) for x in X)\n",
    "# X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "X = pad_sequences(X, maxlen=201)\n",
    "y = data['label_encoded'].values\n",
    "num_classes = len(np.unique(y))  # 동적으로 클래스 개수를 찾음\n",
    "print(num_classes)\n",
    "# Shuffle and split the data while maintaining class balance\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # 모든 시퀀스가 길이 201을 가지도록 합니다.\n",
    "# X_train = pad_sequences(X_train, maxlen=201)\n",
    "# X_test = pad_sequences(X_test, maxlen=201)\n",
    "\n",
    "\n",
    "\n",
    "max_features = 1000\n",
    "maxlen = 201\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "\n",
    "# CNN with max pooling imeplementation \n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test, y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn-kaggle-c.h5')\n",
    "print('Model saved as textcnn-kaggle-c.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d1fd3-bfbc-47dc-9632-6fec1f72a3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "208618f7-6d2e-4b0f-b588-e44d8bef0551",
   "metadata": {},
   "source": [
    "### [Textcnn+Bidirectional(LSTM(64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "18086d80-4eba-4e8e-bfc3-83c21065d07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"bidirectional_2\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.3\u001b[39m))\n\u001b[0;32m---> 50\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBidirectional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adding a Bidirectional LSTM layer\u001b[39;00m\n\u001b[1;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Bidirectional(LSTM(\u001b[38;5;241m64\u001b[39m)))  \u001b[38;5;66;03m# Adding another Bidirectional LSTM layer\u001b[39;00m\n\u001b[1;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique(), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"bidirectional_2\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128600)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your data (replace with your data loading logic)\n",
    "data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "max_sequence_length = max(len(x) for x in X)\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "y = data['label_encoded'].values\n",
    "\n",
    "\n",
    "# Shuffle and split the data while maintaining class balance\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Model design with Conv1D and Bidirectional LSTM\n",
    "embedding_dim = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = Conv1D(num_filters, filter_size, activation='relu')(model.layers[-1].output)\n",
    "    pool_layer = MaxPooling1D()(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # Adding a Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(64)))  # Adding another Bidirectional LSTM layer\n",
    "\n",
    "model.add(Dense(data['label_encoded'].nunique(), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test, y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn-lstm.h5')\n",
    "print('Model saved as textcnn-lstm.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fd3e1-4506-49b9-9ddc-a8a15ab6867f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "93f6ab37-8c65-4229-88a0-09064967fa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "507/507 [==============================] - 182s 356ms/step - loss: 0.7737 - accuracy: 0.7234 - val_loss: 0.5241 - val_accuracy: 0.7978\n",
      "Epoch 2/20\n",
      "507/507 [==============================] - 179s 353ms/step - loss: 0.4442 - accuracy: 0.8287 - val_loss: 0.4458 - val_accuracy: 0.8189\n",
      "Epoch 3/20\n",
      "507/507 [==============================] - 179s 352ms/step - loss: 0.3394 - accuracy: 0.8695 - val_loss: 0.4143 - val_accuracy: 0.8372\n",
      "Epoch 4/20\n",
      "507/507 [==============================] - 175s 346ms/step - loss: 0.2743 - accuracy: 0.8929 - val_loss: 0.4493 - val_accuracy: 0.8392\n",
      "Epoch 5/20\n",
      "507/507 [==============================] - 175s 345ms/step - loss: 0.2378 - accuracy: 0.9076 - val_loss: 0.4629 - val_accuracy: 0.8386\n",
      "Epoch 6/20\n",
      "507/507 [==============================] - 174s 344ms/step - loss: 0.2066 - accuracy: 0.9182 - val_loss: 0.4820 - val_accuracy: 0.8336\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.3987 - accuracy: 0.8443\n",
      "Test accuracy: 0.8443333506584167\n",
      "282/282 [==============================] - 8s 27ms/step\n",
      "\n",
      "---- METRICS RESULTS FOR TESTING DATA ----\n",
      "\n",
      "Total Rows are:  9000\n",
      "[TESTING] Model Accuracy is:  0.8443333333333334\n",
      "[TESTING] Testing Report: \n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "Automatically_Searching_Infor       0.99      1.00      0.99       224\n",
      "         Cross_Site_Scripting       0.89      0.66      0.76       270\n",
      "           Directory_Indexing       0.98      0.94      0.96        84\n",
      "                    HOST_Scan       0.86      0.84      0.85      1263\n",
      "           Leakage_Through_NW       1.00      0.98      0.99       191\n",
      "              Path_Disclosure       0.89      0.82      0.85       955\n",
      "                SQL_Injection       0.91      0.78      0.84       679\n",
      "         System_Cmd_Execution       0.76      0.78      0.77      1961\n",
      "           Vulnerability_Scan       0.84      0.90      0.87      3373\n",
      "\n",
      "                     accuracy                           0.84      9000\n",
      "                    macro avg       0.90      0.86      0.88      9000\n",
      "                 weighted avg       0.85      0.84      0.84      9000\n",
      "\n",
      "Model saved as textcnn-cleanplus.h5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle  # Import shuffle for balanced splitting\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "# data = pd.read_csv('A-Track-cleanplus.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "max_sequence_length = max(len(x) for x in X)\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "y = data['label_encoded'].values  # Use label_encoded instead of one-hot encoding\n",
    "\n",
    "# Split the data into training and testing sets while maintaining class balance\n",
    "X, y = shuffle(X, y, random_state=42)  # Shuffle the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Model design with MaxPooling1D\n",
    "embedding_dim = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = Conv1D(num_filters, filter_size, activation='relu')(model.layers[-1].output)\n",
    "    pool_layer = MaxPooling1D()(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "if len(conv_layers) > 1:\n",
    "    model.add(Dense(num_filters * len(conv_layers), activation='relu'))\n",
    "else:\n",
    "    model.add(Dense(num_filters, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(data['label_encoded'].nunique(), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model-cleanplus.h5', save_best_only=True)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test, y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn-cleanplus.h5')\n",
    "print('Model saved as textcnn-cleanplus.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7db09e-371b-46ad-b98b-17eef0a9790d",
   "metadata": {},
   "source": [
    "## CONV1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23037ce-676d-4985-88fb-e45f7121df46",
   "metadata": {
    "tags": []
   },
   "source": [
    "[reference]\n",
    "\n",
    "https://chat.openai.com/share/99c938a2-8ef6-4930-89a9-f9cdaea92007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4adde-8447-4313-b5d1-1cf1eac93fa7",
   "metadata": {},
   "source": [
    "[try01]textcnn.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d312e-7b64-4c32-aeb6-ca12cbbf8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nuumpy as np\n",
    "data = pd.read_csv('A-Track-cleanplus.csv')  # Replace with your CSV file path\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1edb97f7-a610-4da5-8b5b-07d395201f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv1d_24\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16900)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39mmax_words, output_dim\u001b[38;5;241m=\u001b[39membedding_dim, input_length\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filter_size \u001b[38;5;129;01min\u001b[39;00m filter_sizes:\n\u001b[0;32m---> 38\u001b[0m     conv_layer \u001b[38;5;241m=\u001b[39m \u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     pool_layer \u001b[38;5;241m=\u001b[39m MaxPooling1D()(conv_layer)\n\u001b[1;32m     40\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Flatten())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/input_spec.py:250\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    248\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         )\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv1d_24\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16900)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "# data = pd.read_csv('A-Track-cleanplus.csv')  # Replace with your CSV file path\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "y = pd.get_dummies(data['label_encoded']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model design with MaxPooling1D\n",
    "embedding_dim = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=X.shape[1]))\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = Conv1D(num_filters, filter_size, activation='relu')(model.layers[-1].output)\n",
    "    pool_layer = MaxPooling1D()(conv_layer)\n",
    "    model.add(Flatten())\n",
    "    \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Adding Dropout for regularization\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test.argmax(axis=1), y_pred_test.argmax(axis=1), target_names=[label_names[i] for i in range(len(label_names))])\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn.h5')\n",
    "print('Model saved as textcnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ce3a8-fe94-4af4-a28e-c3dde58db4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "# data = pd.read_csv('A-Track-cleanplus.csv')  # Replace with your CSV file path\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['payload'])\n",
    "X = tokenizer.texts_to_sequences(df['payload'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "y = pd.get_dummies(df['label_encoded']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model design with Dropout\n",
    "embedding_dim = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=X.shape[1]))\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = Conv1D(num_filters, filter_size, activation='relu')(model.layers[-1].output)\n",
    "    pool_layer = MaxPooling1D()(conv_layer)\n",
    "    model.add(pool_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Adding Dropout for regularization\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test.argmax(axis=1), y_pred_test.argmax(axis=1), target_names=[label_names[i] for i in range(len(label_names))])\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn.h5')\n",
    "print('Model saved as textcnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac17e5-8f53-48d6-827a-f64cdba1f336",
   "metadata": {},
   "source": [
    "[try02] textcnn01.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a60603-12a1-480d-9f83-e06ba7656b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "maxlen = 100  # Sequence length\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "\n",
    "y = pd.get_dummies(data['label_encoded']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model design with Conv1D and GlobalMaxPooling1D\n",
    "embedding_dim = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
    "for filter_size in filter_sizes:\n",
    "    model.add(Conv1D(num_filters, filter_size, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                    validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test.argmax(axis=1), y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn01.h5')\n",
    "print('Model saved as textcnn01.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e21778-8285-4165-a82f-f978bb811f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data\n",
    "# data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "maxlen = 100  # Sequence length\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['payload'])\n",
    "X = tokenizer.texts_to_sequences(df['payload'])\n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "\n",
    "y = pd.get_dummies(df['label_encoded']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modified model architecture\n",
    "embedding_dim = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    model.add(Conv1D(num_filters, filter_size, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                    validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test.argmax(axis=1), y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn02.h5')\n",
    "print('Model saved as textcnn02.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80a26a-0b65-4981-933a-5f826b2d7c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "y = pd.get_dummies(data['label_encoded']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model design with Dropout\n",
    "embedding_dim = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = 128\n",
    "\n",
    "inputs = Input(shape=(X.shape[1],))\n",
    "embedding = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=X.shape[1])(inputs)\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = Conv1D(num_filters, filter_size, activation='relu')(embedding)\n",
    "    pool_layer = MaxPooling1D()(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "\n",
    "concat = Flatten()(conv_layers[-1]) if len(filter_sizes) > 1 else Flatten()(conv_layers[0])\n",
    "concat = Dropout(0.5)(concat)\n",
    "output = Dense(9, activation='softmax')(concat)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test.argmax(axis=1), y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('textcnn01.h5')\n",
    "print('Model saved as textcnn01.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29943ed-2d93-43b3-bda8-6bf0de20d073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239e4a93-4f2d-40ea-bd66-34f7e3bcd396",
   "metadata": {},
   "source": [
    "### CONV2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadacc5-509d-4080-be79-710e225ff14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import re\n",
    "# Load the data\n",
    "data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "\n",
    "# 데이터 정제 함수\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 및 공백 제거\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # HTML 태그 제거 (필요 시)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Payload 열에 대해 데이터 정제\n",
    "data['payload'] = data['payload'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the text\n",
    "# max_words = 10000\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "y = pd.get_dummies(data['label_encoded']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model design with Conv2D layers\n",
    "embedding_dim = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = 128\n",
    "\n",
    "inputs = Input(shape=(X.shape[1],))\n",
    "embedding = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=X.shape[1])(inputs)\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    reshape = Reshape((X.shape[1], embedding_dim, 1))(embedding)\n",
    "    conv_layer = Conv2D(num_filters, (filter_size, embedding_dim), activation='relu')(reshape)\n",
    "    pool_layer = MaxPooling2D(pool_size=(X.shape[1] - filter_size + 1, 1))(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "\n",
    "concat = Concatenate(axis=1)(conv_layers) if len(filter_sizes) > 1 else conv_layers[0]\n",
    "flatten = Flatten()(concat)\n",
    "flatten = Dropout(0.5)(flatten)\n",
    "output = Dense(9, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# epochs = 20\n",
    "epochs = 10\n",
    "# batch_size = 64\n",
    "batch_size = 128\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test.argmax(axis=1), y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('conv2d_textcnn.h5')\n",
    "print('Model saved as conv2d_textcnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15eed10-bfbe-41d9-82d8-3e906668e9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import re\n",
    "# Load the data\n",
    "data = pd.read_csv('../dataset/A-Track-clean.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_action'])\n",
    "\n",
    "\n",
    "# 데이터 정제 함수\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 및 공백 제거\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # HTML 태그 제거 (필요 시)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Payload 열에 대해 데이터 정제\n",
    "data['payload'] = data['payload'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the text\n",
    "# max_words = 10000\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')  # 구두점 제거 필터 추가\n",
    "tokenizer.fit_on_texts(data['payload'])\n",
    "X = tokenizer.texts_to_sequences(data['payload'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "y = pd.get_dummies(data['label_encoded']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model design with Conv2D layers\n",
    "embedding_dim = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = 128\n",
    "\n",
    "inputs = Input(shape=(X.shape[1],))\n",
    "embedding = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=X.shape[1])(inputs)\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    reshape = Reshape((X.shape[1], embedding_dim, 1))(embedding)\n",
    "    conv_layer = Conv2D(num_filters, (filter_size, embedding_dim), activation='relu')(reshape)\n",
    "    pool_layer = MaxPooling2D(pool_size=(X.shape[1] - filter_size + 1, 1))(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "\n",
    "concat = Concatenate(axis=1)(conv_layers) if len(filter_sizes) > 1 else conv_layers[0]\n",
    "flatten = Flatten()(concat)\n",
    "flatten = Dropout(0.5)(flatten)\n",
    "output = Dense(9, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model training with EarlyStopping and ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# epochs = 20\n",
    "epochs = 10\n",
    "# batch_size = 64\n",
    "batch_size = 128\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Model evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "test_rep = classification_report(y_test.argmax(axis=1), y_pred_test.argmax(axis=1), target_names=label_encoder.classes_)\n",
    "\n",
    "print()\n",
    "print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "print()\n",
    "print(\"Total Rows are: \", X_test.shape[0])\n",
    "print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "print('[TESTING] Testing Report: ')\n",
    "print(test_rep)\n",
    "\n",
    "# Save the model\n",
    "model.save('conv2d_textcnn.h5')\n",
    "print('Model saved as conv2d_textcnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc61c1a-b8e7-4adc-925c-563a8e11bb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc8a6c-8513-4a40-8a5a-3e92c2ea4c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aae16-fc17-4984-b397-bf74c42335f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e440194-111e-438e-bd4e-4883b8eca305",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 17:55:10.193948: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 17:55:10.651559: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2023-10-18 17:55:10.651688: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2023-10-18 17:55:10.755205: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from keras.preprocessing.text import Tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from keras.preprocessing.sequence import pad_sequences\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# 데이터 세트 준비\n",
    "df = pd.read_csv('../dataset/A-Track-clean.csv')\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = df['payload'].values\n",
    "y_train = df['label_action'].values\n",
    "\n",
    "# 데이터의 불균형 조정\n",
    "oversample = SMOTE()\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)\n",
    "\n",
    "# 텍스트 토큰화\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# 텍스트 패딩\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=100)\n",
    "\n",
    "# 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "# 모델 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "# 모델 평가\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a6d3b-9e15-47c2-bc83-6fd19a3909c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211ec9f-a8f2-4d5b-852d-6a1d37b7de3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.11.0 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/tensorflow-2.11.0-cpu-py39-ubuntu20.04-sagemaker-v1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
